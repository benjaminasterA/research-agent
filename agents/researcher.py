"""
Researcher Agent
ì›¹ ê²€ìƒ‰ ë° ì •ë³´ ìˆ˜ì§‘ ì—ì´ì „íŠ¸
"""

import os
from typing import List, Dict, Any
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from tools.web_search import search_web

load_dotenv()


class ResearcherAgent:
    """ì›¹ ê²€ìƒ‰ ë° ì •ë³´ ìˆ˜ì§‘ ì—ì´ì „íŠ¸"""
    
    def __init__(self, model_name: str = None):
        self.llm = ChatOpenAI(
            model=model_name or os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0
        )
        
        self.summary_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì •ë³´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.
í•­ìƒ ì¶œì²˜ URLì„ í•¨ê»˜ ê¸°ë¡í•˜ì„¸ìš”."""),
            ("human", """ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ '{query}'ì— ê´€í•œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

{search_results}

ìš”ì•½ í˜•ì‹:
- í•µì‹¬ í¬ì¸íŠ¸ë¥¼ bullet pointë¡œ ì •ë¦¬
- ê° í¬ì¸íŠ¸ ë’¤ì— [ì¶œì²˜: URL] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ í‘œê¸°
- ìµœëŒ€ 5ê°œ í¬ì¸íŠ¸ë¡œ ìš”ì•½""")
        ])
    
    def search_and_collect(
        self,
        queries: List[str],
        max_results_per_query: int = 3
    ) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ì‹¤í–‰ ë° ì •ë³´ ìˆ˜ì§‘
        
        Args:
            queries: ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡
            max_results_per_query: ì¿¼ë¦¬ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            ìˆ˜ì§‘ëœ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        all_results = []
        all_sources = []
        gathered_info = []
        
        for query in queries:
            print(f"   ğŸ” ê²€ìƒ‰ ì¤‘: {query}")
            
            try:
                results = search_web(query, max_results=max_results_per_query)
                
                for result in results:
                    all_results.append(result)
                    all_sources.append({
                        "title": result.get("title", ""),
                        "url": result.get("url", ""),
                        "query": query
                    })
                
                # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
                if results:
                    summary = self._summarize_results(query, results)
                    gathered_info.append(summary)
                    
            except Exception as e:
                print(f"   âš ï¸  ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
        
        return {
            "search_results": all_results,
            "sources": all_sources,
            "gathered_info": gathered_info
        }
    
    def _summarize_results(self, query: str, results: List[Dict]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½"""
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        results_text = ""
        for i, r in enumerate(results, 1):
            results_text += f"\n[{i}] {r.get('title', 'No title')}\n"
            results_text += f"URL: {r.get('url', 'No URL')}\n"
            results_text += f"ë‚´ìš©: {r.get('content', 'No content')[:500]}\n"
        
        try:
            chain = self.summary_prompt | self.llm
            response = chain.invoke({
                "query": query,
                "search_results": results_text
            })
            return f"### {query}\n\n{response.content}"
        except Exception as e:
            # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìš”ì•½
            return f"### {query}\n\n" + "\n".join(
                f"- {r.get('title', 'N/A')} [ì¶œì²˜: {r.get('url', 'N/A')}]"
                for r in results[:3]
            )


def execute_research(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    LangGraph ë…¸ë“œ í•¨ìˆ˜: ë¦¬ì„œì¹˜ ì‹¤í–‰
    
    Args:
        state: í˜„ì¬ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    print("\nğŸ” ë¦¬ì„œì¹˜ ì‹¤í–‰ ì¤‘...")
    
    researcher = ResearcherAgent()
    queries = state.get("search_queries", [])
    
    if not queries:
        return {
            "errors": ["ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."],
            "current_step": "research_failed"
        }
    
    results = researcher.search_and_collect(queries)
    
    print(f"   âœ… {len(results['search_results'])}ê°œ ê²°ê³¼ ìˆ˜ì§‘ë¨")
    print(f"   ğŸ“š {len(results['sources'])}ê°œ ì¶œì²˜ ê¸°ë¡ë¨")
    
    return {
        "search_results": results["search_results"],
        "sources": results["sources"],
        "gathered_info": results["gathered_info"],
        "current_step": "research_complete"
    }
